<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>zsz1997 </title>
<style>
    .word{background:#E4FFE9;width:250px;margin:50px auto;padding:20px;font-family:"microsoft yahei";}
    /* 强制不换行 */
    .nowrap{white-space:nowrap;}
    /* 允许单词内断句，首先会尝试挪到下一行，看看下一行的宽度够不够，
    不够的话就进行单词内的断句 */
    .breakword{word-wrap: break-word;}
    /* 断句时，不会把长单词挪到下一行，而是直接进行单词内的断句 */
    .breakAll{word-break:break-all;}            
    /* 超出部分显示省略号 */
    .ellipsis{text-overflow:ellipsis;overflow:hidden;}
    .divcss5 span{ font-size:20px}
    .auto_align{text-align:justify;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto;}
</style>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">MENU</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="publications.html">Publications</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Shengzhou Zhong </h1>
</div>
<h3>2021</h3>
  
<ul>
<p style="width:1400px" class="auto_align">
  <span>1</span>. Zhenyuan Ning, Chao Tu, Xiaohui Di, Qianjin Feng, and <b>Yu Zhang</b>.
  Deep cross-view co-regularized representation learning for glioma subtype identification. 
  <i style="color:#3333CC">Medical Image Analysis (MedIA).</i>
  <a href="https://www.sciencedirect.com/science/article/pii/S1361841521002061">[PDF]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  The new subtypes of diffuse gliomas are recognized by the World Health Organization (WHO) on the basis of genotypes, e.g., isocitrate dehydrogenase and chromosome arms 1p/19q, in addition to the histologic phenotype. Glioma subtype identification can provide valid guidances for both risk-benefit assessment and clinical decision. The feature representations of gliomas in magnetic resonance imaging (MRI) have been prevalent for revealing underlying subtype status. However, since gliomas are highly heterogeneous tumors with quite variable imaging phenotypes, learning discriminative feature representations in MRI for gliomas remains challenging. In this paper, we propose a deep cross-view co-regularized representation learning framework for glioma subtype identification, in which view representation learning and multiple constraints are integrated into a unified paradigm. Specifically, we first learn latent view-specific representations based on cross-view images generated from MRI via a bi-directional mapping connecting original imaging space and latent space, and view-correlated regularizer and output-consistent regularizer in the latent space are employed to explore view correlation and derive view consistency, respectively. We further learn view-sharable representations which can explore complementary information of multiple views by projecting the view-specific representations into a holistically shared space and enhancing via adversary learning strategy. Finally, the view-specific and view-sharable representations are incorporated for identifying glioma subtype. Experimental results on multi-site datasets demonstrate the proposed method outperforms several state-of-the-art methods in detection of glioma subtype status.
  </p>
</p>
</ul>
 
 <ul>
<p style="width:1400px" class="auto_align">
  <span>2</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Neural Network Learning Systems (TNNLS).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9556512">[PDF]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  The Cox proportional hazard model has been widely applied to cancer prognosis prediction. Nowadays, multi-modal data, such as histopathological images and gene data, have advanced this field by providing histologic phenotype and genotype information. However, how to efficiently fuse and select the complementary information of high-dimensional multi-modal data remains challenging for Cox model, as it generally does not equip with feature fusion/selection mechanism. Many previous studies typically perform feature fusion/selection in the original feature space before Cox modeling. Alternatively, learning a latent shared feature space that is tailored for Cox model and simultaneously keeps sparsity is desirable. In addition, existing Cox-based models commonly pay little attention to the actual length of the observed time that may help to boost the model's performance. In this article, we propose a novel Cox-driven multi-constraint latent representation learning framework for prognosis analysis with multi-modal data. Specifically, for efficient feature fusion, a multi-modal latent space is learned via a bi-mapping approach under ranking and regression constraints. The ranking constraint utilizes the log-partial likelihood of Cox model to induce learning discriminative representations in a task-oriented manner. Meanwhile, the representations also benefit from regression constraint, which imposes the supervision of specific survival time on representation learning. To improve generalization and alleviate overfitting, we further introduce similarity and sparsity constraints to encourage extra consistency and sparseness. Extensive experiments on three datasets acquired from The Cancer Genome Atlas (TCGA) demonstrate that the proposed method is superior to state-of-the-art Cox-based models.
  </p>
</p>
</ul>
 
<ul>
<p style="width:1400px" class="auto_align">
  <span>3</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

<ul>
<p style="width:1400px" class="auto_align">
  <span>4</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>
 
<ul>
<p style="width:1400px" class="auto_align">
  <span>5</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

<ul>
<p style="width:1400px" class="auto_align">
  <span>6</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

<h3>2020 </h3>
  
<ul>
<p style="width:1400px" class="auto_align">
  <span>1</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>
  
<ul>
<p style="width:1400px" class="auto_align">
  <span>2</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>
  
<ul>
<p style="width:1400px" class="auto_align">
  <span>3</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

<ul>
<p style="width:1400px" class="auto_align">
  <span>4</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

<ul>
<p style="width:1400px" class="auto_align">
  <span>5</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

<h3>2019</h3>

<ul>
<p style="width:1400px" class="auto_align">
  <span>1</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>


<h3>2018</h3>

<ul>
<p style="width:1400px" class="auto_align">
  <span>1</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

<ul>
<p style="width:1400px" class="auto_align">
  <span>2</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

<ul>
<p style="width:1400px" class="auto_align">
  <span>3</span>. Zhenyuan Ning, Shengzhou Zhong, Qianjin Feng, Wufan Chen, and <b>Yu Zhang</b>.
  SMU-Net: Saliency-guided Morphology-aware U-Net for Breast Lesion Segmentation in Ultrasound Image. 
  <i style="color:#3333CC">IEEE Transactions on Medical Imaging (TMI).</i>
  <a href="https://ieeexplore.ieee.org/abstract/document/9551285">[PDF]</a>
  <a href="https://github.com/YuZhang-SMU/Breast-Lesion-Segmentation">[Code]</a>
</p>
<p style="width:1400px" class="auto_align">
  <img src="pic/Fig2.png" align="left" width="500" hspace="5" vspace="5">
  <p style="width:1400px" class="divcss5 auto_align">
  <b>
  <span>
  [Abstract]
  </span>
  </b>
  Deep learning methods, especially convolutional neural networks, have been successfully applied to lesion segmentation in breast ultrasound (BUS) images. However, pattern complexity and intensity similarity between the surrounding tissues (i.e., background) and lesion regions (i.e., foreground) bring challenges for lesion segmentation. Considering that such rich texture information is contained in background, very few methods have tried to explore and exploit background-salient representations for assisting foreground segmentation. Additionally, other characteristics of BUS images, i.e., 1) low-contrast appearance and blurry boundary, and 2) significant shape and position variation of lesions, also increase the difficulty in accurate lesion segmentation. In this paper, we present a saliency-guided morphology-aware U-Net (SMU-Net) for lesion segmentation in BUS images. The SMU-Net is composed of a main network with an additional middle stream and an auxiliary network. Specifically, we first propose generation of saliency maps which incorporate both low-level and high-level image structures, for foreground and background. These saliency maps are then employed to guide the main network and auxiliary network for respectively learning foreground-salient and background-salient representations. Furthermore, we devise an additional middle stream which basically consists of background-assisted fusion, shape-aware, edge-aware and position-aware units. This stream receives the coarse-to-fine representations from the main network and auxiliary network for efficiently fusing the foreground-salient and background-salient features and enhancing the ability of learning morphological information for network. Extensive experiments on five datasets demonstrate higher performance and superior robustness to the scale of dataset than several state-of-the-art deep learning approaches in breast lesion segmentation in ultrasound image.
  </p>
</p>
</ul>

</td>
</tr>
</table>
</body>
</html>
